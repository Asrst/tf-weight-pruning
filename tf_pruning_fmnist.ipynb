{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0-rc0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np # linear algebra\n",
    "import os\n",
    "import pandas as pd # data processing\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, optimizers, metrics\n",
    "import time\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "BASE_PATH = \"/kaggle/input\"\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs shapes\n",
    "img_rows, img_cols = (28, 28)\n",
    "num_input = img_rows * img_cols\n",
    "num_classes = 10\n",
    "batch_size = 64\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n",
      "data shapes: (60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "(xtrain, ytrain), (xval, yval) = datasets.fashion_mnist.load_data()\n",
    "print('data shapes:', xtrain.shape, ytrain.shape, xval.shape, yval.shape)\n",
    "\n",
    "xtrain = tf.convert_to_tensor(xtrain, dtype=tf.float32)/255.\n",
    "xval = tf.convert_to_tensor(xval, dtype=tf.float32)/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((xtrain, ytrain)).batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((xval, yval)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network = Sequential([layers.Dense(1000, activation='relu'),\n",
    "#                       layers.Dense(1000, activation='relu'),\n",
    "#                       layers.Dense(500, activation='relu'),\n",
    "#                       layers.Dense(200, activation='relu'),\n",
    "#                       layers.Dense(10)])\n",
    "\n",
    "# network.build(input_shape=(None, num_input))\n",
    "# # network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FmnistModel(object):\n",
    "    \n",
    "    def __init__(self, output_shapes, param_initializer):\n",
    "        \n",
    "        self.output_shapes = output_shapes\n",
    "        self.initializer = param_initializer\n",
    "        self.trainable_params = []\n",
    "        \n",
    "        # intialize & store the weights for the model\n",
    "        for i in range(len(self.output_shapes)):\n",
    "            weight = self.get_weight(self.output_shapes[i], \n",
    "                                     name='weight_{}'.format(i))\n",
    "            bias= self.get_bias(self.output_shapes[i][-1],\n",
    "                                name='bias_{}'.format(i))\n",
    "            \n",
    "            self.trainable_params.append(weight)\n",
    "            self.trainable_params.append(bias)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        input transformations\n",
    "        \"\"\"\n",
    "        x = self.dense(x, self.trainable_params[0], self.trainable_params[1])\n",
    "        x = self.dense(x, self.trainable_params[2], self.trainable_params[3])\n",
    "        x = self.dense(x, self.trainable_params[4], self.trainable_params[5])\n",
    "        x = self.dense(x, self.trainable_params[6], self.trainable_params[7])\n",
    "        logits = tf.add(tf.matmul(x, self.trainable_params[8]), self.trainable_params[9])\n",
    "        return logits\n",
    "        \n",
    "    def dense(self, x, W, b):\n",
    "        \n",
    "        \"\"\"\n",
    "        A function with operations of simple dense layer\n",
    "        \"\"\"\n",
    "        \n",
    "        # intialize\n",
    "        x_is_sparse, W_is_sparse = False, False\n",
    "#         sparse_limit = tf.constant(0.3)\n",
    "#         # check for sparsity\n",
    "#         if tf.greater(tf.nn.zero_fraction(x), sparse_limit):\n",
    "#             x_is_sparse = True\n",
    "#         if tf.greater(tf.nn.zero_fraction(W), sparse_limit):\n",
    "#             W_is_sparse = True\n",
    "        # matmul x, W\n",
    "        xW = tf.matmul(x, W, a_is_sparse = x_is_sparse, \n",
    "                       b_is_sparse = W_is_sparse)\n",
    "        return tf.nn.relu(tf.add(xW, b))\n",
    "    \n",
    "    def get_weight(self, shape , name):\n",
    "        \"\"\"\n",
    "        to intialize the weights given shape\n",
    "        \"\"\"\n",
    "        return tf.Variable(self.initializer(shape) , name=name)\n",
    "    \n",
    "    def get_bias(self, units, name):\n",
    "        \"\"\"\n",
    "        to intialize the bias with given no.of units\n",
    "        \"\"\"\n",
    "        return tf.Variable(self.initializer([units]), name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define weight intializer\n",
    "initializer = tf.initializers.glorot_uniform()\n",
    "# define no.of classes\n",
    "num_classes = 10\n",
    "# output shapes\n",
    "shapes = [\n",
    "    [ 28*28*1 , 1000 ] , \n",
    "    [ 1000 , 1000 ] ,\n",
    "    [ 1000 , 500 ] , \n",
    "    [ 500 , 200 ] ,\n",
    "    [ 200 , num_classes] ,\n",
    "]\n",
    "\n",
    "# initialize the model with output_shapes & param_intializer\n",
    "network = FmnistModel(output_shapes = shapes, param_initializer = initializer)\n",
    "len(network.trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    loss function\n",
    "    \"\"\"\n",
    "    return tf.reduce_sum(tf.square(y_pred-y_true))\n",
    "\n",
    "# intialize optimizer with lr = 0.01\n",
    "optimizer = optimizers.SGD(lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Loss: 0.15970  Acc: 0.796  Val_Loss: 0.05350  Val_Acc: 0.826  Time(sec): 21.99\n",
      "Epoch: 1  Loss: 0.13152  Acc: 0.847  Val_Loss: 0.04982  Val_Acc: 0.842  Time(sec): 21.58\n",
      "Epoch: 2  Loss: 0.11621  Acc: 0.860  Val_Loss: 0.04669  Val_Acc: 0.849  Time(sec): 21.62\n",
      "Epoch: 3  Loss: 0.10676  Acc: 0.868  Val_Loss: 0.04500  Val_Acc: 0.856  Time(sec): 21.79\n",
      "Epoch: 4  Loss: 0.10064  Acc: 0.874  Val_Loss: 0.04330  Val_Acc: 0.861  Time(sec): 21.73\n",
      "Epoch: 5  Loss: 0.09489  Acc: 0.879  Val_Loss: 0.04182  Val_Acc: 0.863  Time(sec): 21.69\n",
      "Epoch: 6  Loss: 0.09003  Acc: 0.883  Val_Loss: 0.04006  Val_Acc: 0.866  Time(sec): 21.41\n",
      "Epoch: 7  Loss: 0.08711  Acc: 0.887  Val_Loss: 0.03845  Val_Acc: 0.868  Time(sec): 21.28\n",
      "Epoch: 8  Loss: 0.08390  Acc: 0.890  Val_Loss: 0.03677  Val_Acc: 0.870  Time(sec): 21.25\n",
      "Epoch: 9  Loss: 0.08149  Acc: 0.892  Val_Loss: 0.03511  Val_Acc: 0.871  Time(sec): 21.11\n"
     ]
    }
   ],
   "source": [
    "acc_meter = metrics.Accuracy()\n",
    "val_acc = metrics.Accuracy()\n",
    "epochs = 10\n",
    "\n",
    "# iter over epochs\n",
    "for e in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "    # iter over train data\n",
    "    for step, (xt, yt) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # [bs, 28, 28] => [bs, 784]\n",
    "            xt = tf.reshape(xt, (-1, 28*28))\n",
    "            # [bs, 784] => [bs, 10]\n",
    "            y_pred = network(xt)\n",
    "            # [bs] => [bs, 10]\n",
    "            y_true = tf.one_hot(yt, depth=10)\n",
    "            # compute loss\n",
    "            loss = loss_fn(y_pred, y_true)/batch_size\n",
    "        \n",
    "        # calc train accuracy\n",
    "        acc_meter.update_state(tf.argmax(y_pred, axis=1), yt)\n",
    "        # compute grads & apply them\n",
    "        grads = tape.gradient(loss, network.trainable_params)\n",
    "        optimizer.apply_gradients(zip(grads, network.trainable_params))\n",
    "\n",
    "    # iter over val data\n",
    "    for xv, yv in val_dataset:\n",
    "        xv = tf.reshape(xv, (-1, 28*28))\n",
    "        y_pred_val = network(xv)\n",
    "        val_loss = loss_fn(y_pred_val, tf.one_hot(yv, depth=10))/batch_size\n",
    "        val_acc.update_state(tf.argmax(y_pred_val, axis=1), yv)\n",
    "        \n",
    "    epoch_end = time.time()\n",
    "\n",
    "    print('Epoch: %d' %e, ' Loss: %.5f' %float(loss), \n",
    "          ' Acc: %.3f' %acc_meter.result().numpy(), \n",
    "          ' Val_Loss: %.5f' %float(val_loss), \n",
    "          ' Val_Acc: %.3f' %val_acc.result().numpy(),\n",
    "          ' Time(sec): %.2f' %(epoch_end-epoch_start))\n",
    "    \n",
    "    # reset states of acc meters\n",
    "    acc_meter.reset_states()\n",
    "    val_acc.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "def weight_pruning(w, s):\n",
    "    \n",
    "    \"\"\"Performs pruning on a weight matrix w:\n",
    "\n",
    "    1. Compute absolute value of all elements.\n",
    "    2. The indices of the top k% elements according to their absolute values are selected.\n",
    "    3. A new tensor is formed with indices of topK% elements set to 1.\n",
    "    4. The new tensor will be used as mask & multiplied with the original weights\n",
    "\n",
    "    Args:(w: tf.Variable, s: float)\n",
    "    ------\n",
    "        w: The weight matrix.\n",
    "        k: The percentage of values (units) that should be pruned from the matrix.\n",
    "\n",
    "    Returns: tf.Variable\n",
    "    -------\n",
    "        The pruned weight matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    # store the original w shape\n",
    "    w_shape = tf.shape(w)\n",
    "    # calc % of weights to retain (notice multiplication with 1-s) & type cast to int32\n",
    "    s = tf.cast(tf.size(w, out_type=tf.float32)*tf.constant(1-s), dtype=tf.int32)\n",
    "    # flatten w\n",
    "    w_reshaped = tf.reshape(w, [-1])\n",
    "    # get indices to keep only top s% weights\n",
    "    _, indices = tf.nn.top_k(tf.abs(w_reshaped), s, sorted=True)\n",
    "    # make a mask with top indices values = 1\n",
    "    mask = tf.scatter_nd(tf.reshape(indices, [-1, 1]),\n",
    "                         tf.ones([s], tf.float32), tf.shape(w_reshaped),\n",
    "                         name = 'pruning_mask')\n",
    "    # multiply, reshape, assign & return the weight\n",
    "    return w.assign(tf.reshape(w_reshaped * mask, w_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Loss: 0.07948  Acc: 0.895  Val_Loss: 0.04976  Val_Acc: 0.858  Time(sec): 21.94\n",
      "Epoch: 1  Loss: 0.09410  Acc: 0.889  Val_Loss: 0.03721  Val_Acc: 0.869  Time(sec): 16.31\n",
      "Epoch: 2  Loss: 0.09023  Acc: 0.891  Val_Loss: 0.03664  Val_Acc: 0.871  Time(sec): 16.53\n",
      "Epoch: 3  Loss: 0.08793  Acc: 0.892  Val_Loss: 0.03661  Val_Acc: 0.872  Time(sec): 16.76\n",
      "Epoch: 4  Loss: 0.08628  Acc: 0.892  Val_Loss: 0.03660  Val_Acc: 0.872  Time(sec): 16.40\n",
      "Epoch: 5  Loss: 0.08489  Acc: 0.893  Val_Loss: 0.03653  Val_Acc: 0.872  Time(sec): 16.39\n",
      "Epoch: 6  Loss: 0.08376  Acc: 0.893  Val_Loss: 0.03641  Val_Acc: 0.872  Time(sec): 16.38\n",
      "Epoch: 7  Loss: 0.08271  Acc: 0.893  Val_Loss: 0.03630  Val_Acc: 0.872  Time(sec): 16.48\n",
      "Epoch: 8  Loss: 0.08177  Acc: 0.893  Val_Loss: 0.03620  Val_Acc: 0.873  Time(sec): 16.51\n",
      "Epoch: 9  Loss: 0.08091  Acc: 0.894  Val_Loss: 0.03610  Val_Acc: 0.873  Time(sec): 16.47\n"
     ]
    }
   ],
   "source": [
    "# intialize the metris & constants\n",
    "acc_meter = metrics.Accuracy()\n",
    "val_acc = metrics.Accuracy()\n",
    "epochs = 10\n",
    "total_params = len(network.trainable_params)\n",
    "\n",
    "# iter over epochs\n",
    "for e in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "    # iter over train data\n",
    "    for step, (xt, yt) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # [bs, 28, 28] => [bs, 784]\n",
    "            xt = tf.reshape(xt, (-1, 28*28))\n",
    "            # [bs, 784] => [bs, 10]\n",
    "            y_pred = network(xt)\n",
    "            # [bs] => [bs, 10]\n",
    "            y_true = tf.one_hot(yt, depth=10)\n",
    "            # compute loss\n",
    "            loss = loss_fn(y_pred, y_true)/batch_size\n",
    "\n",
    "        # calc train accuracy\n",
    "        acc_meter.update_state(tf.argmax(y_pred, axis=1), yt)\n",
    "        # compute grads & apply them\n",
    "        grads = tape.gradient(loss, network.trainable_params)\n",
    "        optimizer.apply_gradients(zip(grads, network.trainable_params))\n",
    "        \n",
    "    # pruning weights after end of the epoch\n",
    "    for i in range(0, total_params-2, 2):\n",
    "        network.trainable_params[i] = weight_pruning(network.trainable_params[i], s = 0.4)\n",
    "\n",
    "    # iter over val data\n",
    "    for xv, yv in val_dataset:\n",
    "        xv = tf.reshape(xv, (-1, 28*28))\n",
    "        y_pred_val = network(xv)\n",
    "        val_loss = loss_fn(y_pred_val, tf.one_hot(yv, depth=10))/batch_size\n",
    "        val_acc.update_state(tf.argmax(y_pred_val, axis=1), yv)\n",
    "        \n",
    "    epoch_end = time.time()\n",
    "    \n",
    "    # print the epoch results\n",
    "    print('Epoch: %d' %e, ' Loss: %.5f' %float(loss), \n",
    "          ' Acc: %.3f' %acc_meter.result().numpy(), \n",
    "          ' Val_Loss: %.5f' %float(val_loss), \n",
    "          ' Val_Acc: %.3f' %val_acc.result().numpy(),\n",
    "          ' Time(sec): %.2f' %(epoch_end-epoch_start))\n",
    "    \n",
    "    # reset states of acc meters\n",
    "    acc_meter.reset_states()\n",
    "    val_acc.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As network & training time is already small in our case, Sparse matmul is taking the overhead for conversion & the time taken for epoch is actually more than incase of dense - So, guess not an option in this case.\n",
    "- For stepwise pruning, again time gain is outweight by purning operation which will occur starting from 1000th step of every epoch.\n",
    "- There is about 20-25% time gain (after 1st epoch), when using dense matmul & weights are pruned (s = 0.4) after each epoch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
